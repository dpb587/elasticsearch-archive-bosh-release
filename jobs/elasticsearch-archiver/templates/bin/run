#!/bin/bash

set -eu

es="<%= p('elasticsearch_archiver.elasticsearch') %>"

export PATH="/var/vcap/packages/elasticsearch-archiver/bin:$PATH"

REM () {
  /bin/echo $( date -u +"%Y-%m-%dT%H:%M:%SZ" ) "$@"
}

REM 'starting'

export TMPDIR=/var/vcap/data/elasticsearch-archiver
[ ! -e "$TMPDIR" ] || rm -fr "$TMPDIR"
mkdir "$TMPDIR"

export GNUPGHOME=$TMPDIR/.gpg
mkdir "$GNUPGHOME"
chmod -R 700 "$GNUPGHOME"

cat > "$GNUPGHOME/import_key" <<EOF
<%= p('elasticsearch_archiver.gpg.public_key') %>
EOF

gpg --quiet --import "$GNUPGHOME/import_key"
RECIPIENT=$( gpg --batch --with-colons "$GNUPGHOME/import_key" | head -n1 | cut -d: -f5 )

mc --quiet --json config host add mc \
  "<%= p('elasticsearch_archiver.s3.endpoint') %>" \
  "<%= p('elasticsearch_archiver.s3.access_key') %>" \
  "<%= p('elasticsearch_archiver.s3.secret_key') %>" \
  > /dev/null

for index_name in $( curl -s http://$es/_cat/indices?h=index | sort -r | egrep -v '^\.elasticsearch-archiver' ) ; do
  REM "$index_name: starting"

  TS=$( date -u +"%Y%m%dT%H%M%SZ" )

  index_state=$( curl -s http://$es/_cat/indices?h=index,status | grep "^$index_name " | awk '{ print $2 }' )
  
  if [ "close" == "$index_state" ] ; then
    <% if p('elasticsearch_archiver.ignore_closed') %>
      REM "$index_name: closed"

      continue
    <% end %>

    REM "$index_name: opening"

    curl -sX POST http://$es/$index_name/_open \
      > /dev/null

    status=red

    while [ "$status" != "green" ] ; do
      sleep 2
      
      status=$( curl -s http://$es/_cat/indices?h=index,health | grep "^$index_name " | awk '{ print $2 }' )
    done
  fi

  existing_translog=$( curl -s "http://$es/.elasticsearch-archiver/state/$index_name/_source" | jq -j -r '.translog' )
  
  <% if p('elasticsearch_archiver.optimize_index') %>
    segments=$( wget -qO- "http://$es/$index_name/_segments" | jq '.indices | to_entries[0].value.shards | to_entries | map(.value[0].segments | keys | length) | map(select(1 < .)) | add // 0' )
    
    if [ "0" != "$segments" ] ; then
      REM "$index_name: optimizing"
    
      curl -sX POST "http://$es/$index_name/_optimize?max_num_segments=1" \
        > /dev/null
    fi
  <% end %>

  translog_operations=$( curl -s "http://$es/$index_name/_stats?level=shards" | jq -r '[ .indices | .[.|keys[0]].shards | to_entries | sort_by(.key)[].value[].translog.operations ] | add // 0' )
  
  if [ "0" != "$translog_operations" ] ; then
    REM "$index_name: flushing"
    
    curl -sX POST "http://$es/$index_name/_flush/synced" \
      > /dev/null
  fi

  new_translog=$( curl -s "http://$es/$index_name/_stats?level=shards" | jq -c -r '.indices | .[ . | keys[0] ].shards | to_entries | sort_by(.key) | map(.value | map(select(.routing.primary))[0] ) | map( .commit.user_data.translog_id + "+" + ( .translog.operations | tostring ) ) | join(";")' )

  REM "$index_name: translog: was $existing_translog"
  
  if [ "$new_translog" == "$existing_translog" ] ; then
    REM "$index_name: unchanged"
  else
    REM "$index_name: translog: now $new_translog"
  
    REM "$index_name: dumping"
  
    elasticdump \
      --input="http://$es/$index_name" \
      --output="$TMPDIR/mapping-$TS.json" \
      --type=mapping \
      > /dev/null
  
    elasticdump \
      --input="http://$es/$index_name" \
      --output="$TMPDIR/data-$TS.json" \
      --limit=10240 \
      --type=data \
      > /dev/null

    REM "$index_name:" $( wc -l "$TMPDIR/data-$TS.json" | awk '{ print $1 }' ) "documents"

    REM "$index_name: compressing"

    xz "$TMPDIR/data-$TS.json"
    xz "$TMPDIR/mapping-$TS.json"

    REM "$index_name: encrypting"

    gpg --trust-model always --recipient "$RECIPIENT" --encrypt "$TMPDIR/mapping-$TS.json.xz"
    rm "$TMPDIR/mapping-$TS.json.xz"

    gpg --trust-model always --recipient "$RECIPIENT" --encrypt "$TMPDIR/data-$TS.json.xz"
    rm "$TMPDIR/data-$TS.json.xz"
  
    REM "$index_name: uploading"

    success=false
    
    for retry in {1..3} ; do
      mc --quiet --json cp \
        "$TMPDIR/mapping-$TS.json.xz.gpg" \
        "mc/<%= p('elasticsearch_archiver.s3.bucket') %>/<%= p('elasticsearch_archiver.s3.prefix') %>$index_name/mapping-$TS.json.xz.gpg" \
        > /dev/null \
        || continue
      success=true
      break
    done
    
    [ "true" == "$success" ] || exit 1

    rm "$TMPDIR/mapping-$TS.json.xz.gpg"

    success=false
    
    for retry in {1..3} ; do
      mc --quiet --json cp \
        "$TMPDIR/data-$TS.json.xz.gpg" \
        "mc/<%= p('elasticsearch_archiver.s3.bucket') %>/<%= p('elasticsearch_archiver.s3.prefix') %>$index_name/data-$TS.json.xz.gpg" \
        > /dev/null \
        || continue
      success=true
      break
    done
    
    [ "true" == "$success" ] || exit 1

    rm "$TMPDIR/data-$TS.json.xz.gpg"

    jq -c -n --arg translog "$new_translog" '{"translog":$translog}' \
      | curl -sX PUT -d@- "http://$es/.elasticsearch-archiver/state/$index_name" \
      > /dev/null
  fi

  if [ "close" == "$index_state" ] ; then
    REM "$index_name: closing"

    curl -sX POST http://$es/$index_name/_close \
      > /dev/null
  fi

  REM "$index_name: finished"
done

REM 'cleaning'

rm -fr "$GNUPGHOME"
rm -fr "$HOME/.mc"

REM 'finished'
